{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "#from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import datetime as dt\n",
    "import time\n",
    "import glob\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "tf.enable_eager_execution()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_autoencoder_outputs(encoder, n, dims):\n",
    "#     #decoded_imgs = enconder.predict(x_test)\n",
    "#     x_test_encoded = vae.predict(x_test, batch_size = batch_size)\n",
    "#     # number of example digits to show\n",
    "#     n = 5\n",
    "#     plt.figure(figsize=(20, 9))\n",
    "#     for i in range(n):\n",
    "#         # plot original image\n",
    "#         ax = plt.subplot(2, n, i + 1)\n",
    "#         plt.imshow(x_test[i].reshape(*dims))\n",
    "#         #plt.gray()\n",
    "#         ax.get_xaxis().set_visible(False)\n",
    "#         ax.get_yaxis().set_visible(False)\n",
    "#         if i == n/2:\n",
    "#             ax.set_title('Original Images')\n",
    "\n",
    "#         # plot reconstruction \n",
    "#         ax = plt.subplot(2, n, i + 1 + n)\n",
    "#         plt.imshow(x_test_encoded[i].reshape(*dims))\n",
    "#         #plt.gray()\n",
    "#         ax.get_xaxis().set_visible(False)\n",
    "#         ax.get_yaxis().set_visible(False)\n",
    "#         if i == n/2:\n",
    "#             ax.set_title('Reconstructed Images')\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_loss(history):\n",
    "#     historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     historydf.plot(ylim=(0, historydf.values.max()))\n",
    "#     plt.title('Loss: %.3f' % history.history['loss'][-1])\n",
    "    \n",
    "# def plot_compare_histories(history_list, name_list, plot_accuracy=True):\n",
    "#     dflist = []\n",
    "#     min_epoch = len(history_list[0].epoch)\n",
    "#     losses = []\n",
    "#     for history in history_list:\n",
    "#         h = {key: val for key, val in history.history.items() if not key.startswith('val_')}\n",
    "#         dflist.append(pd.DataFrame(h, index=history.epoch))\n",
    "#         min_epoch = min(min_epoch, len(history.epoch))\n",
    "#         losses.append(h['loss'][-1])\n",
    "\n",
    "#     historydf = pd.concat(dflist, axis=1)\n",
    "\n",
    "#     metrics = dflist[0].columns\n",
    "#     idx = pd.MultiIndex.from_product([name_list, metrics], names=['model', 'metric'])\n",
    "#     historydf.columns = idx\n",
    "    \n",
    "#     plt.figure(figsize=(6, 8))\n",
    "\n",
    "#     ax = plt.subplot(211)\n",
    "#     historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "#     plt.title(\"Training Loss: \" + ' vs '.join([str(round(x, 3)) for x in losses]))\n",
    "    \n",
    "#     if plot_accuracy:\n",
    "#         ax = plt.subplot(212)\n",
    "#         historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "#         plt.title(\"Accuracy\")\n",
    "#         plt.xlabel(\"Epochs\")\n",
    "    \n",
    "#     plt.xlim(0, min_epoch-1)\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import whole N-MNIST Dataset\n",
    "def load_NMNIST(path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    xs_train = []\n",
    "    ys_train = []\n",
    "    xs_test = []\n",
    "    ys_test = []\n",
    "\n",
    "    for class_index in range(0, 10):\n",
    "        for (root, dirs, dat_files) in os.walk('{0}/n_Train_3/{1}'.format(path, str(class_index))):\n",
    "            for file in dat_files:\n",
    "                single_X = np.fromfile('{0}/n_Train_3/{1}/{2}'.format(path, str(class_index), file), dtype=np.int32)\n",
    "                xs_train.append(single_X)\n",
    "                ys_train.append(class_index)\n",
    "\n",
    "        for (root, dirs, dat_files) in os.walk('{0}/n_Test_3/{1}'.format(path, str(class_index))):\n",
    "            for file in dat_files:\n",
    "                xs_test.append(np.fromfile('{0}/n_Test_3/{1}/{2}'.format(path, str(class_index), file), dtype=np.int32))\n",
    "                ys_test.append(class_index)\n",
    "\n",
    "    Xtr = np.array(xs_train)\n",
    "    Xtr_reshaped = Xtr.reshape(60000, 34, 34, 2)\n",
    "    Ytr = np.array(ys_train)\n",
    "    Xte = np.array(xs_test)\n",
    "    Xte_reshaped = Xte.reshape(10000, 34, 34, 2)\n",
    "    Yte = np.array(ys_test)\n",
    "\n",
    "    return Xtr_reshaped, Ytr, Xte_reshaped, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 34, 34, 2)\n",
      "Training labels shape:  (60000,)\n",
      "Test data shape:  (10000, 34, 34, 2)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "dataset_class_path = '/Users/ching/Desktop/Project/autoencoder_nmnist/NMNIST/datasets'\n",
    "x_train_1, y_train, x_test_1, y_test = load_NMNIST(dataset_class_path)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', x_train_1.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test_1.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "# 34 x 34 x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dataset from range -1 to 1. Since the maxima and minima lies btw 15 so we divide it by 15.\n",
    "x_train = (x_train_1.astype('float32'))\n",
    "x_test = (x_test_1.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 34, 34, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train!=0]+=30\n",
    "x_test[x_test!=0]+=30\n",
    "#print(x_train.shape)\n",
    "#print(max(x_train[0])) #Test the maximum value after flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train/45\n",
    "x_train[:] = [x / 45 for x in x_train]\n",
    "#x_test = x_test/45\n",
    "x_test[:] = [x / 45 for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train[x_train>0]=1\n",
    "# x_train[x_train<0]=-1\n",
    "# x_test[x_test>0]=1\n",
    "# x_test[x_test<0]=-1\n",
    "# #print(x_train.shape)\n",
    "# #print(max(x_train[0])) #Test the maximum value after flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 60000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TEST_BUF = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(x_test).shuffle(TEST_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(34, 34, 2)),\n",
    "                \n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32, kernel_size=(3, 3), strides=(2, 2), activation=tf.nn.relu),\n",
    "                \n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64, kernel_size=(3, 3), strides=(2, 2), activation=tf.nn.relu),\n",
    "                \n",
    "                tf.keras.layers.Flatten(),\n",
    "                \n",
    "                # No activation\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.generative_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "                \n",
    "                tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                \n",
    "                tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                \n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64,\n",
    "                    kernel_size=(3, 3),\n",
    "                    strides=(2, 2),\n",
    "                    padding=\"SAME\",\n",
    "                    activation=tf.nn.relu),\n",
    "                \n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32,\n",
    "                    kernel_size=(3, 3),\n",
    "                    strides=(2, 2),\n",
    "                    padding=\"SAME\",\n",
    "                    activation=tf.nn.relu),\n",
    "                \n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=(3, 3), strides=(2, 2), padding=\"SAME\"),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random_normal(shape=(100, self.latent_dim))\n",
    "            return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random_normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    \n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "def compute_gradients(model, x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    return tape.gradient(loss, model.trainable_variables), loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "def apply_gradients(optimizer, gradients, variables, global_step=None):\n",
    "    optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "latent_dim = 50\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "random_vector_for_generation = tf.random_normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "model = CVAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please provide as model inputs either a single array or a list of arrays. You passed: x=(<BatchDataset shapes: (?, 34, 34, 2), types: tf.float32>, <BatchDataset shapes: (?, 34, 34, 2), types: tf.float32>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9eb6108f79cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'steps_per_epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1535\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1536\u001b[1;33m         validation_split=validation_split)\n\u001b[0m\u001b[0;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[1;32m--> 992\u001b[1;33m                                                      class_weight, batch_size)\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   1013\u001b[0m                    tensor_util.is_tensor(v) for v in x):\n\u001b[0;32m   1014\u001b[0m           raise ValueError('Please provide as model inputs either a single '\n\u001b[1;32m-> 1015\u001b[1;33m                            'array or a list of arrays. You passed: x=' + str(x))\n\u001b[0m\u001b[0;32m   1016\u001b[0m         \u001b[0mall_inputs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide as model inputs either a single array or a list of arrays. You passed: x=(<BatchDataset shapes: (?, 34, 34, 2), types: tf.float32>, <BatchDataset shapes: (?, 34, 34, 2), types: tf.float32>)"
     ]
    }
   ],
   "source": [
    "model.fit((train_dataset, train_dataset), epochs=epochs, batch_size=BATCH_SIZE, steps_per_epoch = 60000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
