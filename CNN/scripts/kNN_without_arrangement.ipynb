{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from past.builtins import xrange\n",
    "import math\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor(object):\n",
    "    \"\"\" a kNN classifier with L2 distance \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the classifier. For k-nearest neighbors this is just \n",
    "        memorizing the training data.\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (num_train, D) containing the training data\n",
    "          consisting of num_train samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing the training labels, where\n",
    "             y[i] is the label for X[i].\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X, k=1, num_loops=0):\n",
    "        \"\"\"\n",
    "        Predict labels for test data using this classifier.\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (num_test, D) containing test data consisting\n",
    "             of num_test samples each of dimension D.\n",
    "        - k: The number of nearest neighbors that vote for the predicted labels.\n",
    "        - num_loops: Determines which implementation to use to compute distances\n",
    "          between training points and testing points.\n",
    "        Returns:\n",
    "        - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "          test data, where y[i] is the predicted label for the test point X[i].  \n",
    "        \"\"\"\n",
    "        if num_loops == 0:\n",
    "            dists = self.compute_distances_L2(X)\n",
    "\n",
    "        return self.predict_labels(dists, k=k)\n",
    "\n",
    "  \n",
    "    def compute_distances_L1(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using no explicit loops.\n",
    "        Input / Output: Same as compute_distances_two_loops\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train)) \n",
    "        for i in range(num_test):\n",
    "            # using the L1 distance (sum of absolute value differences)\n",
    "            dists[i,:] = np.sum(np.abs(self.X_train - X[i,:]), axis = 1)\n",
    "\n",
    "        return dists\n",
    "    \n",
    "    def compute_distances_L2(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using no explicit loops.\n",
    "        Input / Output: Same as compute_distances_two_loops\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train)) \n",
    "\n",
    "\n",
    "        # L2 distance vectorized.\n",
    "        X_squared = np.sum(X**2,axis=1)\n",
    "        Y_squared = np.sum(self.X_train**2,axis=1)\n",
    "        XY = np.dot(X, self.X_train.T)\n",
    "\n",
    "        # Expand L2 distance formula to get L2(X,Y) = sqrt((X-Y)^2) = sqrt(X^2 + Y^2 -2XY)\n",
    "        dists = np.sqrt(X_squared[:,np.newaxis] + Y_squared -2*XY)\n",
    "        return dists\n",
    "\n",
    "    def predict_labels(self, dists, k=1):\n",
    "        \"\"\"\n",
    "        Given a matrix of distances between test points and training points,\n",
    "        predict a label for each test point.\n",
    "        Inputs:\n",
    "        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
    "          gives the distance betwen the ith test point and the jth training point.\n",
    "        Returns:\n",
    "        - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "          test data, where y[i] is the predicted label for the test point X[i].  \n",
    "        \"\"\"\n",
    "        num_test = dists.shape[0]\n",
    "        y_pred = np.zeros(num_test)\n",
    "        for i in range(num_test):\n",
    "            # A list of length k storing the labels of the k nearest neighbors to\n",
    "            # the ith test point.\n",
    "            closest_y = []\n",
    "\n",
    "            # Select a test row.\n",
    "            test_row = dists[i,:]\n",
    "\n",
    "            # np.argsort returns indices of sorted input.\n",
    "            sorted_row = np.argsort(test_row)\n",
    "\n",
    "            # Get the k closest indices.\n",
    "            closest_y = self.y_train[sorted_row[0:k]]\n",
    "\n",
    "            # Find the most occuring index in our closest k.\n",
    "            y_pred[i] = np.argmax(np.bincount(closest_y))\n",
    "\n",
    "            return y_pred\n",
    "    \"\"\"# Let's compare how fast the implementations are\n",
    "    def time_function(f, *args):\n",
    "        \"\"\"\n",
    "        #Call a function f with args and return the time (in seconds) that it took to execute.\n",
    "        \"\"\"\n",
    "        import time\n",
    "        tic = time.time()\n",
    "        f(*args)\n",
    "        toc = time.time()\n",
    "        return toc - tic\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import whole N-MNIST Dataset\n",
    "def load_NMNIST(path):\n",
    "    xs_train = []\n",
    "    ys_train = []\n",
    "    xs_test = []\n",
    "    ys_test = []\n",
    "\n",
    "    for class_index in range(0, 10):\n",
    "        for (root, dirs, dat_files) in os.walk('{0}/n_Train_2/{1}'.format(path, str(class_index))):\n",
    "            for file in dat_files:\n",
    "                single_X = np.fromfile('{0}/n_Train_2/{1}/{2}'.format(path, str(class_index), file), dtype=np.int32)\n",
    "                xs_train.append(single_X)\n",
    "                ys_train.append(class_index)\n",
    "\n",
    "        for (root, dirs, dat_files) in os.walk('{0}/n_Test_2/{1}'.format(path, str(class_index))):\n",
    "            for file in dat_files:\n",
    "                xs_test.append(np.fromfile('{0}/n_Test_2/{1}/{2}'.format(path, str(class_index), file), dtype=np.int32))\n",
    "                ys_test.append(class_index)\n",
    "\n",
    "    Xtr = np.array(xs_train)\n",
    "    Ytr = np.array(ys_train)\n",
    "    Xte = np.array(xs_test)\n",
    "    Yte = np.array(ys_test)\n",
    "    \n",
    "    \"\"\"Turn Xtr -> 68x34 here since \"\"\"\n",
    "    \n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 2312)\n",
      "Training labels shape:  (60000,)\n",
      "Test data shape:  (10000, 2312)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Load data\n",
    "data_set_path = 'C:/Users/Justin/Documents/LowPowerActionRecognition/CNN/datasets'\n",
    "data = load_NMNIST(data_set_path)\n",
    "\n",
    "#initialise data\n",
    "\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "X_test = data[2]\n",
    "y_test = data[3]\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data visualisation\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train are: (60000, 2312)\n"
     ]
    }
   ],
   "source": [
    "print('X_train are:',X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Visualize some examples from the dataset.\\n# We show a few examples of training images from each class.\\nclasses = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\\nnum_classes = len(classes)\\nsamples_per_class = 3\\nfor y, cls in enumerate(classes):\\n    idxs = np.flatnonzero(y_train == y)\\n    idxs = np.random.choice(idxs, samples_per_class, replace=False)\\n    for i, idx in enumerate(idxs):\\n        plt_idx = i * num_classes + y + 1\\n        plt.subplot(samples_per_class, num_classes, plt_idx)\\n        #X_train[idx] = X_train[idx].reshape(34,34,2) # Ptr_rows becomes [number of training sets] x 2312\\n        plt.imshow(X_train[idx])\\n        plt.axis('off')\\n        if i == 0:\\n            plt.title(cls)\\nplt.show()\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 3\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        #X_train[idx] = X_train[idx].reshape(34,34,2) # Ptr_rows becomes [number of training sets] x 2312\n",
    "        plt.imshow(X_train[idx])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data for more efficient code execution in this exercise\n",
    "num_training = 6000 #60000\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "num_test = 1000 #10000\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare how fast the implementations are\n",
    "def time_function(f, *args):\n",
    "    \"\"\"\n",
    "    #Call a function f with args and return the time (in seconds) that it took to execute.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    tic = time.time()\n",
    "    f(*args)\n",
    "    toc = time.time()\n",
    "    return toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2312) (1000, 2312)\n"
     ]
    }
   ],
   "source": [
    "# flatten out all images to be one-dimensional\n",
    "#X_train = X_train.reshape(X_train.shape[0], 34 * 34 * 2) # Ptr_rows becomes [number of training sets] x 2312\n",
    "#X_test = X_test.reshape(X_test.shape[0], 34 * 34 * 2) # Pte_rows becomes [number of testing sets] x 2312\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from kNN_classifier import KNearestNeighbor\n",
    "\n",
    "# Create a kNN classifier instance. \n",
    "# Remember that training a kNN classifier is a noop: \n",
    "# the Classifier simply remembers the data and does no further processing \n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6000)\n",
      "For L1 distance at K=1, Got 980 / 1000 correct => accuracy: 0.980000\n",
      "L1 distance version took 76.772196 seconds\n"
     ]
    }
   ],
   "source": [
    "dists = classifier.compute_distances_L1(X_test)\n",
    "\n",
    "#Check dists dimension\n",
    "print(dists.shape)\n",
    "\n",
    "# Now implement the function predict_labels and run the code below:\n",
    "# We use k = 1 (which is Nearest Neighbor) for L1 distance.\n",
    "y_test_pred = classifier.predict_labels(dists, k=1)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('For L1 distance at K=1, Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
    "L1_time = time_function(classifier.compute_distances_L1, X_test)\n",
    "print('L1 distance version took %f seconds' % L1_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6000)\n",
      "For L2 distance at K=1, Got 980 / 1000 correct => accuracy: 0.980000\n",
      "L2 distance version took 6.928474 seconds\n"
     ]
    }
   ],
   "source": [
    "dists = classifier.compute_distances_L2(X_test)\n",
    "\n",
    "#Check dists dimension\n",
    "print(dists.shape)\n",
    "\n",
    "# Now implement the function predict_labels and run the code below:\n",
    "# We use k = 1 (which is Nearest Neighbor) for L1 distance.\n",
    "y_test_pred = classifier.predict_labels(dists, k=1)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('For L2 distance at K=1, Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
    "L2_time = time_function(classifier.compute_distances_L2, X_test)\n",
    "print('L2 distance version took %f seconds' % L2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For L2 distance at k=5, Got 980 / 1000 correct => accuracy: 0.980000\n",
      "L2 distance version took 8.082817 seconds\n"
     ]
    }
   ],
   "source": [
    "#k=5 test for L2\n",
    "\n",
    "#Calling L1 distance\n",
    "dists = classifier.compute_distances_L2(X_test)\n",
    "\n",
    "y_test_pred = classifier.predict_labels(dists, k=5)\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('For L2 distance at k=5, Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
    "L2_time = time_function(classifier.compute_distances_L2, X_test)\n",
    "print('L2 distance version took %f seconds' % L2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For L1 distance at k=5, Got 980 / 1000 correct => accuracy: 0.980000\n",
      "L1 distance version took 75.926649 seconds\n"
     ]
    }
   ],
   "source": [
    "#k=5 test for L1\n",
    "\n",
    "#Calling L1 distance\n",
    "dists = classifier.compute_distances_L1(X_test)\n",
    "\n",
    "y_test_pred = classifier.predict_labels(dists, k=5)\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('For L1 distance at k=5, Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
    "L1_time = time_function(classifier.compute_distances_L1, X_test)\n",
    "print('L1 distance version took %f seconds' % L1_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.300000\n",
      "k = 1, accuracy = 0.223000\n",
      "mean for k=1 is 0.296150\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.300000\n",
      "k = 3, accuracy = 0.223000\n",
      "mean for k=3 is 0.296150\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.300000\n",
      "k = 5, accuracy = 0.223000\n",
      "mean for k=5 is 0.296150\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.300000\n",
      "k = 8, accuracy = 0.223000\n",
      "mean for k=8 is 0.296150\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.300000\n",
      "k = 10, accuracy = 0.223000\n",
      "mean for k=10 is 0.296150\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.300000\n",
      "k = 12, accuracy = 0.223000\n",
      "mean for k=12 is 0.296150\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.300000\n",
      "k = 15, accuracy = 0.223000\n",
      "mean for k=15 is 0.296150\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.300000\n",
      "k = 20, accuracy = 0.223000\n",
      "mean for k=20 is 0.296150\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.300000\n",
      "k = 50, accuracy = 0.223000\n",
      "mean for k=50 is 0.296150\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.300000\n",
      "k = 100, accuracy = 0.223000\n",
      "mean for k=100 is 0.296150\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for L2\n",
    "num_folds = 20\n",
    "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "\n",
    "X_train_folds = []\n",
    "y_train_folds = []\n",
    "\n",
    "# Split up the training data into folds \n",
    "# y_train_folds[i] is the label vector for the points in X_train_folds[i].     \n",
    "\n",
    "X_train_folds = np.array(np.array_split(X_train, num_folds))\n",
    "y_train_folds = np.array(np.array_split(y_train, num_folds))\n",
    "\n",
    "\n",
    "# A dictionary holding the accuracies for different values of k that we find\n",
    "# when running cross-validation. After running cross-validation,\n",
    "# k_to_accuracies[k] should be a list of length num_folds giving the different\n",
    "# accuracy values that we found when using that value of k.\n",
    "k_to_accuracies = {}\n",
    "\n",
    "# Perform k-fold cross validation to find the best value of k. For each        \n",
    "# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   \n",
    "# where in each case you use all but one of the folds as training data and the \n",
    "# last fold as a validation set. Store the accuracies for all fold and all     \n",
    "# values of k in the k_to_accuracies dictionary.                               \n",
    "\n",
    "for k in k_choices:\n",
    "    for n in xrange(num_folds):\n",
    "        combinat = [x for x in xrange(num_folds) if x != n] \n",
    "        x_training_dat = np.concatenate(X_train_folds[combinat])\n",
    "        y_training_dat = np.concatenate(y_train_folds[combinat])\n",
    "        classifier_k = KNearestNeighbor()\n",
    "        classifier_k.train(x_training_dat, y_training_dat)\n",
    "        y_cross_validation_pred = classifier_k.predict_labels(X_train_folds[n], k)\n",
    "        num_correct = np.sum(y_cross_validation_pred == y_train_folds[n])\n",
    "        accuracy = float(num_correct) / num_test\n",
    "        k_to_accuracies.setdefault(k, []).append(accuracy)\n",
    "\n",
    "# Print out the computed accuracies\n",
    "for k in sorted(k_to_accuracies):\n",
    "    for accuracy in k_to_accuracies[k]:\n",
    "        #print('k = %d, accuracy = '%f' % (k, accuracy))\n",
    "        print('k = %d, accuracy = %f' % (k, accuracy))\n",
    "    print('mean for k=%d is %f' % (k, np.mean(k_to_accuracies[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAHwCAYAAAD5BSj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xu8HXV97//Xm1wIgoBKKOFmQNFq/VEvWyreKlALVQF7PApWj2KrtMdSsV6O0FpFelpvteqpHhWtPVAvqIBC1BoVsdKKlkRoLDeFeCEQJAIJ14SEfH5/rNm4s7MvKyRr7y/Zr+fjsR57zXd9Z+az1jDwZma+M6kqJEmS1KYdprsASZIkjc+wJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5qkGSnJaUk+1b3fP8mdSWZN1vcBruuKJM95oPNPhSSV5NHTXYekzRnWJPUtyR8kWdIFm5VJ/iXJM6e7rq1VVT+vql2q6r6tXVaS/5fkf49a/m9U1be3dtmSZibDmqS+JHkD8AHgb4FfA/YH/i9w7Dj9Z09ddZK0/TKsSZpUkt2A04E/rarzququqlpfVYuq6s1dn9OSnJPkU0luB05IsmOSDyS5sXt9IMmOXf89knw5yeoktya5OMkO3WdvSXJDkjuSXJPkiHHq+lqSk0a1/WeS/9a9/2CS65PcnmRpkmeNs5yF3WnA2d30AUn+tVv/N4A9RvX/QpKbkqxJ8p0kv9G1nwi8DPhf3dHHRV37T5P8Tvd+ot/kOUlWJHljkpu7o5evmmC77J3kgu73uzbJa0Z8dlqSzyc5q/seVyQZGm9Zo5b7zO53O6yf/pIGy7AmqR+HAvOAL07S71jgHGB34NPAXwJPA54I/CZwCPDWru8bgRXAfHpH6v4CqCSPBU4CnlpVDwWOBH46zvo+A7x0eCLJ44FHAl/pmi7t1v3wru8Xkszr4/t+BlhKL6T9NfDKUZ//C3AQsCfwg+67UlVndO/f051WPXqMZU/0mwDsBewG7AP8EfDhJA8bp87P0vsN9wb+O/C3o4LtMcDZ9LbHBcCHJvviSY7slvuiqrposv6SBs+wJqkfjwB+WVUbJul3SVV9qao2VtU99I4ynV5VN1fVKuAdwP/o+q4HFgCP7I7SXVy9hxXfB+wIPD7JnKr6aVVdN876vgg8Mckju+mXAedV1TqAqvpUVd1SVRuq6n3dch870RdIsj/wVOCvqmpdVX0HWDSyT1V9sqru6NZzGvCb3dHHfkz0mwz/Lqd3v8lXgTvHqjnJfsAzgbdU1dqquhz4xKhl/VtVfbW7Fu+f6YXDibwYOAN4XlX9R5/fR9KAGdYk9eMWYI8+rkO7ftT03sDPRkz/rGsDeC9wLfD1JMuTnAJQVdcCr6cXgm5OcnaSvQG6U4vDr/2r6g56R9GO75Z5PN1Rrq7/G5Nc1Z2uXE3viNUmpzTHsDdwW1XdNaru4WXOSvKuJNd1p3t/2n002XJHLn+83wTgllGh+G5gl3GWc2v3G4xc1j4jpm8atZx5k2zD1wOfr6ofTtBH0hQzrEnqxyXAWuCFk/SrUdM30jstOWz/ro3uyNQbq+pA4GjgDcOn8KrqM1X1zG7eAt7dte8y4vXzbpmfBV6a5FBgJ+AigO76tLcALwEeVlW7A2uATPIdVgIPS7LzqLqH/QG9072/Qy/8Lezah5c7+jcYbdzfZAvdCDw8yUNHLeuGB7CsYS8GXpjk9VuxDEnbmGFN0qSqag3wNnrXT70wyUOSzEnye0neM8GsnwXemmR+kj26ZQzf2+wFSR6dJMDt9E5/3pfksUkO7y66Xwvc0302nq/SCz+nA5+rqo1d+0OBDcAqYHaStwG79vFdfwYsAd6RZG56tyYZee3ZQ4F19I42PoTe6NiRfgEcOMEqxv1NtkRVXQ98F3hnknlJDqZ3jdunJ55zQjcCRwCvS/LarViOpG3IsCapL1X198Ab6F0Mv4reKc+TgC9NMNv/phd8lgE/pHcx/vA9yA4CvknvmqxLgP/b3YtsR+BdwC/pncbbk97gg/HqWgecR+9I12dGfLSY3kCAH9E7PbiWzU/TjucPgN8CbgXeDpw14rOzuuXdAFwJfG/UvP9I73q71UnG+m0m+k221EvpHdm7kd71e2+vqm88wGUBvXvO0Qtsb0ny6q1ZlqRtI73reSVJktQij6xJkiQ1zLAmSZLUMMOaJElSwwYa1pIc1T0q5trheyiN0eclSa7sHoXyma7tiUku6dqWJTlukHVKkiS1amADDJLMojcK67n0HodyKfDSqrpyRJ+DgM8Dh1fVbUn2rKqbkzwGqKr6cXczzKXA46pq9UCKlSRJatRkdyPfGocA11bVcoAkZ9O7keSVI/q8BvhwVd0GUFU3d39/NNyhqm5McjO95weOG9b22GOPWrhw4bb+DpIkSdvc0qVLf1lV8/vpO8iwtg+b3tNoBb37Fo30GIAk/w7MAk6rqq+N7JDkEGAusNmzAZOcCJwIsP/++7NkyZJtVrwkSdKgJPnZ5L16BnnN2liPdBl9znU2vRtjPofezR0/kWT3+xeQLKD38OFXjbgr+a8WVnVGVQ1V1dD8+X2FU0mSpAeVQYa1FcB+I6b3ZfPn360Azq+q9VX1E+AaeuGNJLvSe0DzW6tq9B3CJUmSZoRBhrVLgYOSHJBkLnA8cMGoPl8CDgPonpH3GGB51/+LwFlV9YUB1ihJktS0gYW1qtpA77mBi4GrgM9X1RVJTk9yTNdtMXBLkiuBi4A3V9UtwEuAZwMnJLm8ez1xULVKkiS1art5NujQ0FA5wECSJD0YJFlaVUP99PUJBpIkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrG2B4z52Ccd97JIp7/dgqGdrv0tr69HM5T9j0szV6v5vWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGjbQsJbkqCTXJLk2ySnj9HlJkiuTXJHkMyPaX5nkx93rlYOsU5IkqVWzB7XgJLOADwPPBVYAlya5oKquHNHnIOBU4BlVdVuSPbv2hwNvB4aAApZ28942qHonc8ApX6G69wtP+QoBfvKu52/e8bSHwbq/6N4fBewAp21e9sFnHsxdv3gNAP/fmScSwrJXLuu7nqse93jufsaf9N6//w8h4XFXXblZvw//ybe4cZd1978H+NOPHr5Zv/cddzQr9jq6e/83QHjj5xb1VcuKUy5mHXfd/x5g33c9q+/v0q/TTjuNn657bPd+8f1t0rZy4bcezerVJ3XvXw6EIw6/dnqLkjQlFlx0OXNW3wnAXhddToCVhz1xeovqDPLI2iHAtVW1vKruBc4Gjh3V5zXAh4dDWFXd3LUfCXyjqm7tPvsGcNQAa53QyKA2rLr2TZz2MGDjqJ4bu/ZfOfjMg6lRSyyKg888uK96rnrc46FGVVTVax9hOJyNNrr9fccdDWN8w177xIbDWb/tD9R4ocywpm3lwm89mrH2g167pO3ZgosuH/O/8wsuunw6ytnMIMPaPsD1I6ZXdG0jPQZ4TJJ/T/K9JEdtwbxTZvQGHL99dFAbu310UJusffOO4/Qbr33yBW5hu7Q9cj+QZqrW9/6BnQYFMkbb6O89GzgIeA6wL3Bxkif0OS9JTgROBNh///23plZJkqQmDfLI2gpgvxHT+wI3jtHn/KpaX1U/Aa6hF976mZeqOqOqhqpqaP78+du0eEmSpBYMMqxdChyU5IAkc4HjgQtG9fkScBhAkj3onRZdDiwGfjfJw5I8DPjdrm1ajHWYb+z28X7OTdszzhLHa9+84zj9xmuffIFb2C5tj9wPpJmq9b1/YGGtqjYAJ9ELWVcBn6+qK5KcnuSYrtti4JYkVwIXAW+uqluq6lbgr+kFvkuB07u2afGTdz1/sw025mjQ025j859089Ggy165bLNgtiWjQR931ZWbB7MxRoOONepzrPbeqM/Nv2E/o0HHG/W5rUeDOsBAg9Yb9bn5fuBoUGn7t/KwJ4753/lWRoOmHvBF6W0ZGhqqJUuWDHQdx33sEgA+98eHTmm/B0M9W/tdWluPZi7/GZNmrqnc/5Msraqhfvr6BANJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGpaqmu4ZtYmhoqJYsWTLQdTztb7/JXffex+MX7DphvytX3g6wzfpN1Xq2pp6t/S6trUczl/+MSTPXlStvZ+e5s/jeX/zOwNeVZGlVDfXT1yNrkiRJDZs93QU8mDzyETsD8Lk/PnTCfsd97JJt2m+q1rM19Wztd2ltPZq5/GdMmrmG9//WeGRNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWGGNUmSpIYZ1iRJkhpmWJMkSWqYYU2SJKlhhjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJatikYS3JC5IY6iRJkqZBPyHseODHSd6T5HGDLkiSJEm/MmlYq6qXA08CrgP+KcklSU5M8tCBVydJkjTD9XV6s6puB84FzgYWAL8P/CDJn000X5KjklyT5Nokp4zx+QlJViW5vHu9esRn70lyRZKrkvyfJNmibyZJkrQdmD1ZhyRHA38IPAr4Z+CQqro5yUOAq4B/GGe+WcCHgecCK4BLk1xQVVeO6vq5qjpp1LxPB54BHNw1/Rvw28C3+/xekiRJ24VJwxrwYuD9VfWdkY1VdXeSP5xgvkOAa6tqOUCSs4FjgdFhbSwFzAPmAgHmAL/oYz5JkqTtSj+nQd8O/MfwRJKdkiwEqKoLJ5hvH+D6EdMrurbRXpRkWZJzkuzXLfcS4CJgZfdaXFVX9VGrJEnSdqWfsPYFYOOI6fu6tsmMdY1ZjZpeBCysqoOBbwJnAiR5NPA4YF96Ae/wJM/ebAW9gQ5LkixZtWpVHyVJkiQ9uPQT1mZX1b3DE937uX3MtwLYb8T0vsCNIztU1S1Vta6b/DjwlO797wPfq6o7q+pO4F+Ap41eQVWdUVVDVTU0f/78PkqSJEl6cOknrK1KcszwRJJjgV/2Md+lwEFJDkgyl9792i4Y2SHJghGTx9AbsADwc+C3k8xOMofe4AJPg0qSpBmnnwEGfwJ8OsmH6J3avB54xWQzVdWGJCcBi4FZwCer6ookpwNLquoC4HVdENwA3Aqc0M1+DnA48EN6p06/VlWLtuibSZIkbQcmDWtVdR3wtCS7AKmqO/pdeFV9FfjqqLa3jXh/KnDqGPPdB/xxv+uRJEnaXvVzZI0kzwd+A5g3fG/aqjp9gHVJkiSJ/h7k/lHgOODP6J0GfTHwyAHXJUmSJPobYPD0qnoFcFtVvQM4lE1HeUqSJGlA+glra7u/dyfZG1gPHDC4kiRJkjSsn2vWFiXZHXgv8AN6ozM/PtCqJEmSBEwS1pLsAFxYVauBc5N8GZhXVWumpDpJkqQZbsLToFW1EXjfiOl1BjVJkqSp0881a19P8qIM37NDkiRJU6afa9beAOwMbEiylt7tO6qqdh1oZZIkSerrCQYPnYpCJEmStLlJw1qSZ4/VXlXf2fblSJIkaaR+ToO+ecT7ecAhwFJ6D1qXJEnSAPVzGvTokdNJ9gPeM7CKJEmSdL9+RoOOtgJ4wrYuRJIkSZvr55q1f6D31ALohbsnAv85yKIkSZLU0881a0tGvN8AfLaq/n1A9UiSJGmEfsLaOcDaqroPIMmsJA+pqrsHW5okSZL6uWbtQmCnEdM7Ad8cTDmSJEkaqZ+wNq+q7hye6N4/ZHAlSZIkaVg/Ye2uJE8enkjyFOCewZUkSZKkYf1cs/Z64AtJbuymFwDHDa4kSZIkDevnpriXJvl14LH0HuJ+dVWtH3hlkiRJmvw0aJI/BXauqv+qqh8CuyR57eBLkyRJUj/XrL2mqlYPT1TVbcBrBleSJEmShvUT1nZIkuGJJLOAuYMrSZIkScP6GWCwGPh8ko/Se+zUnwBfG2hVkiRJAvoLa28B/hj4n/QGGHwd+MQgi5IkSVJPP6NBNwIf6V6SJEmaQpOGtSQHAe8EHg/MG26vqgMHWJckSZLob4DBP9E7qrYBOAw4C/jnQRYlSZKknn7C2k5VdSGQqvpZVZ0GHD7YsiRJkgT9DTBYm2QH4MdJTgJuAPYcbFmSJEmC/o6svR54CPA64CnAy4FXDrIoSZIk9fT1bNDu7Z3AqwZbjiRJkkbq58iaJEmSpolhTZIkqWGGNUmSpIb1c1Pc+cBrgIUj+1fVHw6uLEmSJEHv3mkTd0i+C1wMLAXuG26vqnMHW9qWGRoaqiVLlkx3GZIkSZNKsrSqhvrp28991h5SVW/ZypokSZL0APRzzdqXkzxv4JVIkiRpM/2EtZPpBba1Se7oXrcPujBJkiT1d1Pch05FIZIkSdpcP9eskeQY4Nnd5Ler6suDK0mSJEnDJj0NmuRd9E6FXtm9Tu7aJEmSNGD9HFl7HvDEqtoIkORM4DLglEEWJkmSpP6fYLD7iPe7DaIQSZIkba6fI2vvBC5LchEQeteunTrQqiRJkgT0Nxr0s0m+DTyVXlh7S1XdNOjCJEmSNMFp0CS/3v19MrAAWAFcD+zdtUmSJGnAJjqy9gbgROB9Y3xWwOEDqUiSJEn3GzesVdWJ3dvfq6q1Iz9LMm+gVUmSJAnobzTod/tskyRJ0jY27pG1JHsB+wA7JXkSvcEFALsCD5mC2iRJkma8ia5ZOxI4AdgX+PsR7XcAfzHAmiRJktSZ6Jq1M4Ezk7yoqs6dwpokSZLU6ec+a+cmeT7wG8C8Ee2nD7IwSZIk9fcg948CxwF/Ru+6tRcDjxxwXZIkSaK/0aBPr6pXALdV1TuAQ4H9BluWJEmSoL+wdk/39+4kewPrgQMGV5IkSZKG9fMg9y8n2R14L/ADek8v+MRAq5IkSRLQ3wCDv+7enpvky8C8qloz2LIkSZIEE98U979N8BlVdd5gSpIkSdKwiY6sHd393RN4OvCtbvow4NuAYU2SJGnAJrop7qsAulOfj6+qld30AuDDU1OeJEnSzNbPaNCFw0Gt8wvgMQOqR5IkSSP0Mxr020kWA5+lNxL0eOCigVYlSZIkoL/RoCd1gw2e1TWdUVVfHGxZkiRJgv6OrA2P/HRAgSRJ0hSb6NYd/1ZVz0xyB73Tn/d/BFRV7Trw6iRJkma4iUaDPrP7+9CpK0eSJEkjjTsaNMnDJ3r1s/AkRyW5Jsm1SU4Z4/MTkqxKcnn3evWIz/ZP8vUkVyW5MsnCB/IFJUmSHswmumZtKb3TnxnjswIOnGjBSWbRux/bc4EVwKVJLqiqK0d1/VxVnTTGIs4C/qaqvpFkF2DjROuTJEnaHk10GvSArVz2IcC1VbUcIMnZwLHA6LC2mSSPB2ZX1Te6Wu7cylokSZIelPq5KS5JHpbkkCTPHn71Mds+wPUjpld0baO9KMmyJOck2a9rewywOsl5SS5L8t7uSN3ouk5MsiTJklWrVvXzVSRJkh5UJg1r3XVk3wEWA+/o/p7Wx7LHO3060iJ6T0g4GPgmcGbXPpvefd3eBDyV3inXEzZbWNUZVTVUVUPz58/voyRJkqQHl36OrJ1MLzD9rKoOA54E9HMYawWw34jpfYEbR3aoqluqal03+XHgKSPmvayqllfVBuBLwJP7WKckSdJ2pZ+wtraq1gIk2bGqrgYe28d8lwIHJTkgyVx6j6m6YGSH7qHww44Brhox78OSDB8uO5w+rnWTJEna3vTzBIMVSXand3TrG0luY9QRsrFU1YYkJ9E7bToL+GRVXZHkdGBJVV0AvC7JMcAG4Fa6U51VdV+SNwEXJgm9kakf3/KvJ0mS9OCWqtGXkU3QOfltYDfga1V178CqegCGhoZqyZIl012GJEnSpJIsraqhfvpOemQtyQfp3Qvtu1X1r1tdnSRJkvrWzzVrPwDe2j2F4L1J+kqBkiRJ2nqThrWqOrOqnkfvJrc/At6d5McDr0ySJEn93RS382jg14GFwNUDqUaSJEmb6OemuMNH0k4HrgCeUlVHD7wySZIk9XXrjp8Ah1bVLwddjCRJkjbVzzVrHx0OaklOG3hFkiRJut+WXLMGvacMSJIkaYpsaVgb6+HskiRJGpAtDWtPmbyLJEmStpV+RoO+J8muSebQezboL5O8fApqkyRJmvH6ObL2u1V1O/ACYAXwGODNA61KkiRJQH9hbU7393nAZ6vq1gHWI0mSpBH6uc/aoiRXA/cAr00yH1g72LIkSZIE/d1n7RTgUGCoqtYDdwHHDrowSZIk9TfA4MXAhqq6L8lbgU8Bew+8MkmSJPV1zdpfVdUdSZ4JHAmcCXxksGVJkiQJ+gtr93V/nw98pKrOB+YOriRJkiQN6yes3ZDkY8BLgK8m2bHP+SRJkrSV+gldLwEWA0dV1Wrg4XifNUmSpCnRz2jQu4HrgCOTnATsWVVfH3hlkiRJ6ms06MnAp4E9u9enkvzZoAuTJElSfzfF/SPgt6rqLoAk7wYuAf5hkIVJkiSpv2vWwq9GhNK9z2DKkSRJ0kj9HFn7J+D7Sb7YTb8Q+MfBlSRJkqRhk4a1qvr7JN8GnknviNqrquqyQRcmSZKkScJakh2AZVX1BOAHU1OSJEmShk14zVpVbQT+M8n+U1SPJEmSRujnmrUFwBVJ/gO4a7ixqo4ZWFWSJEkC+gtr7xh4FZIkSRrTuGEtyaOBX6uqfx3V/mzghkEXJkmSpImvWfsAcMcY7Xd3n0mSJGnAJgprC6tq2ejGqloCLBxYRZIkSbrfRGFt3gSf7bStC5EkSdLmJgprlyZ5zejGJH8ELB1cSZIkSRo20WjQ1wNfTPIyfhXOhoC5wO8PujBJkiRNENaq6hfA05McBjyha/5KVX1rSiqTJElSX88GvQi4aApqkSRJ0igTPm5KkiRJ08uwJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNG2hYS3JUkmuSXJvklDE+PyHJqiSXd69Xj/p81yQ3JPnQIOuUJElq1exBLTjJLODDwHOBFcClSS6oqitHdf1cVZ00zmL+GvjXQdUoSZLUukEeWTsEuLaqllfVvcDZwLH9zpzkKcCvAV8fUH2SJEnNG2RY2we4fsT0iq5ttBclWZbknCT7ASTZAXgf8OYB1idJktS8QYa1jNFWo6YXAQur6mDgm8CZXftrga9W1fVMIMmJSZYkWbJq1aqtLliSJKk1A7tmjd6RtP1GTO8L3DiyQ1XdMmLy48C7u/eHAs9K8lpgF2Bukjur6pRR858BnAEwNDQ0OghKkiQ96A0yrF0KHJTkAOAG4HjgD0Z2SLKgqlZ2k8cAVwFU1ctG9DkBGBod1CRJkmaCgYW1qtqQ5CRgMTAL+GRVXZHkdGBJVV0AvC7JMcAG4FbghEHVI0mS9GCUqu3j7OHQ0FAtWbJkusuQJEmaVJKlVTXUT1+fYCBJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDVs9nQX8GDxpctu4L2Lr+HG1few9+478eYjH8sLn7TP5h2XfR4uPB3WrIDd9oUj3gYHv2Szbl9Z/hU++IMPctNdN7HXzntx8pNP5vkHPr/vetYsWsTN7/8AG1atMbSyAAALYUlEQVSuZPaCBez5569nt6OP3qzfj75/E5ecfx133rqOXR6+I4ce+yge81t7bdbvqosv4uKzz+KOW37JQx+xB886/hU87lmH9VXLXZfdzO2Lf8p9q9cxa/cd2fXIhez8pD37/i79WrZsGRdeeCFr1qxht91244gjjuDggw/e5uvRzLXypvNZft3fsXbdSubtuIADH/UmFux17HSXJWkKnHvTrbxz+UpuWLeefXacw6kHLuBFez18ussCDGt9+dJlN3DqeT/knvX3AXDD6ns49bwfAmwa2JZ9Hha9Dtbf05tec31vGjYJbF9Z/hVO++5prL1vLQAr71rJad89DaCvwLZm0SJW/tXbqLW9+TfceCMr/+ptAJsEth99/yYu+vTVbLh3IwB33rqOiz59NcAmge2qiy/i62d8iA33rgPgjl+u4utnfAhg0sB212U3s/q8H1Pre+u4b/U6Vp/3Y4BtGtiWLVvGokWLWL9+PQBr1qxh0aJFAAY2bRMrbzqfq6/+SzZu7O2/a9fdyNVX/yWAgU3azp1706286ZrruWdjAbBi3XredM31AE0ENk+D9uG9i6+5P6gNu2f9fbx38TWbdrzw9F8FtWHr7+m1j/DBH3zw/qA2bO19a/ngDz7YVz03v/8D9we1YbV2LTe//wObtF1y/nX3B7VhG+7dyCXnX7dJ28Vnn3V/UPtVv3VcfPZZk9Zy++Kf3h/U7q9l/UZuX/zTSefdEhdeeOH9QW3Y+vXrufDCC7fpejRzLb/u7+4PasM2bryH5df93TRVJGmqvHP5yvuD2rB7NhbvXL5ymiralGGtDzeuvqe/9jUrxl7AqPab7rppzG7jtY+2YeXY//CMbr/z1nVj9hvdfsctvxyz33jtI923eux1jNf+QK1Zs2aL2qUttXbd2PvVeO2Sth83rFu/Re1TzbDWh71336m/9t32HXsBo9r32nnza8Ymah9t9oIFfbXv8vAdx+w3uv2hj9hjzH7jtY80a/ex1zFe+wO12267bVG7tKXm7Tj2fjVeu6Ttxz47ztmi9qlmWOvDm498LDvNmbVJ205zZvHmIx+7accj3gZzRgW4OTv12kc4+cknM2/WvE3a5s2ax8lPPrmvevb889eTeZvOn3nz2PPPX79J26HHPorZczfdxLPn7sChxz5qk7ZnHf8KZs/dcVS/HXnW8a+YtJZdj1xI5my6jszZgV2PXDjpvFviiCOOYM6cTXeaOXPmcMQRR2zT9WjmOvBRb2KHHTbdf3fYYScOfNSbpqkiSVPl1AMXsNMO2aRtpx3CqQe28T9rDjDow/AggklHgw4PIphkNOjwIIIHOhp0eBDBZKNBhwcRTDYadHgQwQMZDTo8iGDQo0GHBxE4GlSDMjyIwNGg0swzPIig1dGgqarJez0IDA0N1ZIlS6a7DEmSpEklWVpVQ/309TSoJElSwwxrkiRJDTOsSZIkNWygYS3JUUmuSXJtklPG+PyEJKuSXN69Xt21PzHJJUmuSLIsyXGDrFOSJKlVAxsNmmQW8GHgucAK4NIkF1TVlaO6fq6qThrVdjfwiqr6cZK9gaVJFlfV6kHVK0mS1KJBHlk7BLi2qpZX1b3A2UBfY+Cr6kdV9ePu/Y3AzcD8gVUqSZLUqEGGtX2A60dMr+jaRntRd6rznCT7jf4wySHAXOC6zWeVJEnavg0yrGWMttE3dVsELKyqg4FvAmdusoBkAfDPwKuqauOoeUlyYpIlSZasWrVqG5UtSZLUjkGGtRXAyCNl+wI3juxQVbdU1fATvz8OPGX4syS7Al8B3lpV3xtrBVV1RlUNVdXQ/PmeJZUkSdufQYa1S4GDkhyQZC5wPHDByA7dkbNhxwBXde1zgS8CZ1XVFwZYoyRJUtMGNhq0qjYkOQlYDMwCPllVVyQ5HVhSVRcAr0tyDLABuBU4oZv9JcCzgUckGW47oaouH1S9kiRJLfLZoJIkSVPMZ4NKkiRtJwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ0zrEmSJDXMsCZJktQww5okSVLDDGuSJEkNM6xJkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkmS1DDDmiRJUsMMa5IkSQ1LVU13DdtEklXAz7bxYvcAfrmNl6mt53Zpk9ulTW6XNrld2jVV2+aRVTW/n47bTVgbhCRLqmpouuvQptwubXK7tMnt0ia3S7ta3DaeBpUkSWqYYU2SJKlhhrWJnTHdBWhMbpc2uV3a5HZpk9ulXc1tG69ZkyRJaphH1iRJkhpmWBtDkqOSXJPk2iSnTHc9M1WS/ZJclOSqJFckOblrf3iSbyT5cff3YdNd60yUZFaSy5J8uZs+IMn3u+3yuSRzp7vGmSjJ7knOSXJ1t+8c6j4z/ZL8effvsf9K8tkk89xnpl6STya5Ocl/jWgbc/9Iz//pssCyJE+erroNa6MkmQV8GPg94PHAS5M8fnqrmrE2AG+sqscBTwP+tNsWpwAXVtVBwIXdtKbeycBVI6bfDby/2y63AX80LVXpg8DXqurXgd+kt43cZ6ZRkn2A1wFDVfUEYBZwPO4z0+H/AUeNahtv//g94KDudSLwkSmqcTOGtc0dAlxbVcur6l7gbODYaa5pRqqqlVX1g+79HfT+o7MPve1xZtftTOCF01PhzJVkX+D5wCe66QCHA+d0Xdwu0yDJrsCzgX8EqKp7q2o17jMtmA3slGQ28BBgJe4zU66qvgPcOqp5vP3jWOCs6vkesHuSBVNT6aYMa5vbB7h+xPSKrk3TKMlC4EnA94Ffq6qV0At0wJ7TV9mM9QHgfwEbu+lHAKurakM37X4zPQ4EVgH/1J2i/kSSnXGfmVZVdQPwd8DP6YW0NcBS3GdaMd7+0UweMKxtLmO0OWR2GiXZBTgXeH1V3T7d9cx0SV4A3FxVS0c2j9HV/WbqzQaeDHykqp4E3IWnPKdddw3UscABwN7AzvROsY3mPtOWZv69Zljb3ApgvxHT+wI3TlMtM16SOfSC2qer6ryu+RfDh6K7vzdPV30z1DOAY5L8lN5lAofTO9K2e3eKB9xvpssKYEVVfb+bPodeeHOfmV6/A/ykqlZV1XrgPODpuM+0Yrz9o5k8YFjb3KXAQd0onbn0LgK9YJprmpG666D+Ebiqqv5+xEcXAK/s3r8SOH+qa5vJqurUqtq3qhbS2z++VVUvAy4C/nvXze0yDarqJuD6JI/tmo4ArsR9Zrr9HHhakod0/14b3i7uM20Yb/+4AHhFNyr0acCa4dOlU82b4o4hyfPoHSmYBXyyqv5mmkuakZI8E7gY+CG/ujbqL+hdt/Z5YH96/xJ8cVWNvmBUUyDJc4A3VdULkhxI70jbw4HLgJdX1brprG8mSvJEegM/5gLLgVfR+x9z95lplOQdwHH0RrlfBrya3vVP7jNTKMlngecAewC/AN4OfIkx9o8uWH+I3ujRu4FXVdWSaanbsCZJktQuT4NKkiQ1zLAmSZLUMMOaJElSwwxrkiRJDTOsSZIkNcywJkljSLIwyX9Ndx2SZFiTJElqmGFNkiaR5MDuwehPne5aJM08hjVJmkD36KZz6d29/NLprkfSzDN78i6SNGPNp/ecwBdV1RXTXYykmckja5I0vjXA9cAzprsQSTOXR9YkaXz3Ai8EFie5s6o+M90FSZp5DGuSNIGquivJC4BvJLmrqs6f7pokzSypqumuQZIkSePwmjVJkqSGGdYkSZIaZliTJElqmGFNkiSpYYY1SZKkhhnWJEmSGmZYkyRJaphhTZIkqWH/P244OQU0gp5cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the raw observations\n",
    "for k in k_choices:\n",
    "    accuracies = k_to_accuracies[k]\n",
    "    plt.scatter([k] * len(accuracies), accuracies)\n",
    "\n",
    "# plot the trend line with error bars that correspond to standard deviation\n",
    "accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n",
    "plt.title('Cross-validation on k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the cross-validation results above, choose the best value for k,   \n",
    "# retrain the classifier using all the training data, and test it on the test\n",
    "# data. You should be able to get above 28% accuracy on the test data.\n",
    "best_k = 1\n",
    "\n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test, k=best_k)\n",
    "\n",
    "# Compute and display the accuracy\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
